# Reading List

## Mechanistic Interpretability

### Circuits & Induction Heads

[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)

[In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)

[Induction heads - illustrated — LessWrong](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated)

[Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)

[Thread: Circuits](https://distill.pub/2020/circuits/)

[Understanding neural networks through sparse circuits](https://openai.com/index/understanding-neural-networks-through-sparse-circuits/)

[Towards Automated Circuit Discovery for Mechanistic Interpretability](https://arxiv.org/pdf/2304.14997)

[Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)

[Transcoders enable fine-grained interpretable circuit analysis for language models — AI Alignment Forum](https://www.alignmentforum.org/posts/YmkjnWtZGLbHRbzrP/transcoders-enable-fine-grained-interpretable-circuit)

[Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking](https://arxiv.org/pdf/2402.14811)

### Sparse Autoencoders & Features

[Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)

[Mathematical Models of Computation in Superposition](https://arxiv.org/pdf/2408.05451)

[Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://arxiv.org/pdf/2309.08600)

[An Intuitive Explanation of Sparse Autoencoders for Mechanistic Interpretability of LLMs — LessWrong](https://www.lesswrong.com/posts/CJPqwXoFtgkKPRay8/an-intuitive-explanation-of-sparse-autoencoders-for)

[HDBSCAN is Surprisingly Effective at Finding Interpretable Clusters of the SAE Decoder Matrix — LessWrong](https://www.lesswrong.com/posts/Dc2w5kHXksSBcjNTs/hdbscan-is-surprisingly-effective-at-finding-interpretable)

[Negative Results for SAEs on Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update #2) — LessWrong](https://www.lesswrong.com/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks)

[Unsupervised sentiment neuron](https://openai.com/index/unsupervised-sentiment-neuron/)

[NeurIPS Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://neurips.cc/virtual/2025/loc/san-diego/129759)

### Representation Geometry & Probing

[Transformers represent belief state geometry in their residual stream](https://arxiv.org/abs/2405.15943)

[The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets](https://arxiv.org/pdf/2310.06824)

[Representation Engineering: A Top-Down Approach to AI Transparency](https://arxiv.org/abs/2310.01405)

[The Platonic Representation Hypothesis](https://arxiv.org/pdf/2405.07987)

[REEF: Representation Encoding Fingerprints for Large Language Models](https://arxiv.org/pdf/2410.14273)

[The Bayesian Geometry of Transformer Attention](https://arxiv.org/html/2512.22471v2#bib)

[The Information Geometry of Softmax: Probing and Steering](https://arxiv.org/abs/2602.15293)

[Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin](https://arxiv.org/pdf/2510.06477)

### Interpretability Methods, Theory & Tools

[On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-tracing)

[Stanford CS25: V5 I On the Biology of a Large Language Model, Josh Batson of Anthropic](https://www.youtube.com/watch?v=vRQs7qfIDaU)

[Garcon: Anthropic's interpretability infrastructure](https://transformer-circuits.pub/2021/garcon/index.html)

[Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition](https://arxiv.org/pdf/2501.14926)

[The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/pdf/2507.08802)

[NeurIPS Poster The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://neurips.cc/virtual/2025/loc/san-diego/poster/117348)

[Simplifying Outcomes of Language Model Component Analyses with ELIA](https://arxiv.org/abs/2602.18262v1)

[Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)

[Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research](https://www.arxiv.org/abs/2509.22831)

[JET EXPANSIONS: RESTRUCTURING LLM COMPUTATION FOR MODEL INSPECTION](https://www.google.com/search?q=JET+EXPANSIONS%3A+RESTRUCTURING+LLM+COMPUTATION+FOR+MODEL+INSPECTION&ie=UTF-8&oe=UTF-8&hl=zh-hans-us&client=safari)

[Circuits Updates - July 2024](https://transformer-circuits.pub/2024/july-update/index.html)

## LLM Cognition & Behavior

### Bayesian & Probabilistic Reasoning

[Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective](https://arxiv.org/pdf/2406.00793)

[Bayesian teaching enables probabilistic reasoning in large language models - Nature Communications](https://www.nature.com/articles/s41467-025-67998-6)

[Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models](https://arxiv.org/abs/2503.17523)

[Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)

[Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)

[What Are the Odds? Language Models Are Capable of Probabilistic Reasoning](https://arxiv.org/abs/2406.12830)

[On the attribution of confidence to large language models](https://arxiv.org/abs/2407.08388)

[LLMs are Bayesian, In Expectation, Not in Realization](https://arxiv.org/abs/2507.11768)

[Large Language Models as Discounted Bayesian Filters](https://arxiv.org/abs/2512.18489)

[Reasoning over Uncertain Text by Generative Large Language Models](https://arxiv.org/abs/2402.09614v3)

[Position: LLMs Need a Bayesian Meta-Reasoning Framework for More Robust and Generalizable Reasoning](https://openreview.net/forum?id=RrvhbxO2hd)

[Attention Is Bayesian Inference](https://medium.com/@vishalmisra/attention-is-bayesian-inference-578c25db4501)

### In-Context Learning

[Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)

[NeurIPS Poster Pre-trained Large Language Models Learn to Predict Hidden Markov Models In-context](https://neurips.cc/virtual/2025/loc/san-diego/poster/117142)

[(How) Do Language Models Track State?](https://arxiv.org/abs/2503.02854)

[What learning algorithm is in-context learning? Investigations with linear models](https://arxiv.org/pdf/2211.15661)

[Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression](https://arxiv.org/abs/2306.15063)

[CL-bench: A Benchmark for Context Learning](https://arxiv.org/pdf/2602.03587)

### Reasoning & Chain-of-Thought

[Filler tokens don't allow sequential reasoning — LessWrong](https://www.lesswrong.com/posts/KFkKPbuYCWc9ygpRp/filler-tokens-don-t-allow-sequential-reasoning)

[Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538)

[THOUGHT ANCHORS: WHICH LLM REASONING STEPS MATTER?](https://arxiv.org/pdf/2506.19143)

[Towards a Typology of Strange LLM Chains-of-Thought — LessWrong](https://www.lesswrong.com/posts/qgvSMwRrdqoDMJJnD/towards-a-typology-of-strange-llm-chains-of-thought)

[Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)

[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)

[The pitfalls of next-token prediction](https://arxiv.org/pdf/2403.06963)

### Beliefs, Knowledge & World Models

[Standards for Belief Representations in LLMs](https://arxiv.org/abs/2405.21030)

[Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827)

[What large language models know and what people think they know - Nature Machine Intelligence](https://www.nature.com/articles/s42256-024-00976-7)

[The Internal State of an LLM Knows When It's Lying](https://arxiv.org/pdf/2304.13734)

[How do language models learn facts? Dynamics, curricula and hallucinations](https://arxiv.org/pdf/2503.21676)

[Evaluating the World Model Implicit in a Generative Model](https://arxiv.org/abs/2406.03689)

[Implicit Representations of Meaning in Neural Language Models](https://arxiv.org/pdf/2106.00737)

[Learning is Forgetting; LLM Training As Lossy Compression](https://openreview.net/forum?id=tvDlQj0GZB)

### Theory of Mind & Self-Awareness

[Language Models Represent Beliefs of Self and Others](https://arxiv.org/abs/2402.18496)

[Emergent Introspective Awareness in Large Language Models](https://transformer-circuits.pub/2025/introspection/index.html)

[From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models](https://arxiv.org/html/2506.14224v1)

[CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/pdf/2601.15628)

### Personas, Psychology & Psychometrics

[Role play with large language models - Nature](https://www.nature.com/articles/s41586-023-06647-8)

[The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models](https://arxiv.org/pdf/2601.10387)

[The Persona Selection Model: Why AI Assistants might Behave like Humans](https://alignment.anthropic.com/2026/psm/)

[Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions](https://arxiv.org/abs/2512.12775)

[A psychometric framework for evaluating and shaping personality traits in large language models - Nature Machine Intelligence](https://www.nature.com/articles/s42256-025-01115-6)

[Measuring Human and AI Values Based on Generative Psychometrics with Large Language Models](https://arxiv.org/abs/2409.12106)

[NeurIPS Personality Manipulation as a Cognitive Probe in Large Language Models](https://neurips.cc/virtual/2025/loc/san-diego/129753)

[The empirical structure of psychopathology is represented in large language models - Nature Mental Health](https://www.nature.com/articles/s44220-025-00527-y)

[Discovering Language Model Behaviors with Model-Written Evaluations](https://arxiv.org/abs/2212.09251)

[models have some pretty funny attractor states — LessWrong](https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states)

[AI–AI bias: Large language models favor communications generated by large language models](https://www.pnas.org/doi/10.1073/pnas.2415697122)

[Cultural tendencies in generative AI - Nature Human Behaviour](https://www.nature.com/articles/s41562-025-02242-1)

[LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926v3)

[EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees](https://arxiv.org/pdf/2503.08893)

### Scaling & Training Dynamics

[Densing law of LLMs - Nature Machine Intelligence](https://www.nature.com/articles/s42256-025-01137-0)

[Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/pdf/2507.14805)

[Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment](https://arxiv.org/pdf/2601.10160)

## AI Alignment

### Surveys & Foundational Theory

[AI Alignment: A Comprehensive Survey](https://arxiv.org/pdf/2310.19852)

[The Alignment Problem from a Deep Learning Perspective](https://arxiv.org/abs/2209.00626)

[AI Alignment at Your Discretion](https://arxiv.org/abs/2502.10441)

[Language Models Resist Alignment: Evidence From Data Compression](https://arxiv.org/pdf/2406.06144)

[Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://paulgoelz.de/papers/alignment.pdf)

[AI safety for everyone - Nature Machine Intelligence](https://www.nature.com/articles/s42256-025-01020-y)

[What Does It Mean to Align AI With Human Values? | Quanta Magazine](https://www.quantamagazine.org/what-does-it-mean-to-align-ai-with-human-values-20221213/)

[If you don't feel deeply confused about AGI risk, something's wrong](https://davebanerjee.substack.com/p/if-you-dont-feel-deeply-confused?r=30u9fn&utm_medium=ios&triedRedirect=true)

[Advancing independent research on AI alignment](https://openai.com/index/advancing-independent-research-ai-alignment/)

### Deceptive Alignment & Scheming

[Alignment faking in large language models](https://arxiv.org/abs/2412.14093)

[Scheming AIs: Will AIs fake alignment during training in order to get power?](https://arxiv.org/pdf/2311.08379)

[Frontier Models are Capable of In-context Scheming](https://arxiv.org/pdf/2412.04984)

[Frontier Models are Capable of In-Context Scheming – Apollo Research](https://www.apolloresearch.ai/research/frontier-models-are-capable-of-incontext-scheming/)

[Reasoning models don't always say what they think](https://www.anthropic.com/research/reasoning-models-dont-say-think)

[Why Do Some Language Models Fake Alignment While Others Don't?](https://arxiv.org/abs/2506.18032)

[Detecting and reducing scheming in AI models](https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/)

[Shutdown resistance in reasoning models](https://palisaderesearch.org/blog/shutdown-resistance)

[Claude Sonnet 3.7 (often) knows when it's in alignment evaluations — LessWrong](https://www.lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment)

[Sidestepping Evaluation Awareness and Anticipating Misalignment with Production Evaluations](https://alignment.openai.com/prod-evals/)

[Steering Evaluation-Aware Models to Act Like They Are Deployed — AI Alignment Forum](https://www.alignmentforum.org/posts/peKrvZ6t9PSCzoQDa/steering-evaluation-aware-models-to-act-like-they-are)

[Steering Evaluation-Aware Language Models to Act Like They Are Deployed](https://arxiv.org/abs/2510.20487)

[Sandbagging with misaligned action — Chain-of-Thought Transcript — Anti-Scheming](https://www.antischeming.ai/cot-transcripts/figure-2-sandbag-model-graded-cot)

[How likely is deceptive alignment? — AI Alignment Forum](https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment)

[Does SGD Produce Deceptive Alignment? — AI Alignment Forum](https://www.alignmentforum.org/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment)

[Apollo Research reveals AI scheming is already here](https://medium.com/@ZombieCodeKill/apollo-research-reveals-ai-scheming-is-already-here-776790e77f36)

[Thinking about reasoning models made me less worried about scheming — LessWrong](https://www.lesswrong.com/posts/HYCGA2p4bBG68Yufh/thinking-about-reasoning-models-made-me-less-worried-about)

[Did Claude 3 Opus align itself via gradient hacking? — LessWrong](https://www.lesswrong.com/posts/ioZxrP7BhS5ArK59w/did-claude-3-opus-align-itself-via-gradient-hacking)

[Gradient hacking — LessWrong](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking)

[Stress Testing Deliberative Alignment for Anti-Scheming Training](https://www.arxiv.org/abs/2509.15541)

[Auditing language models for hidden objectives](https://arxiv.org/abs/2503.10965)

[EMERGENT DECEPTIVE BEHAVIORS IN REWARD-OPTIMIZING LLMS](https://openreview.net/pdf?id=BQfRA3tqt9)

### Emergent Misalignment

[From shortcuts to sabotage: natural emergent misalignment from reward hacking](https://www.anthropic.com/research/emergent-misalignment-reward-hacking)

[Toward understanding and preventing misalignment generalization](https://openai.com/index/emergent-misalignment/)

[Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424)

[NATURAL EMERGENT MISALIGNMENT FROM REWARDHACKING IN PRODUCTION RL](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf)

[Natural Emergent Misalignment from Reward Hacking in Production RL](https://arxiv.org/pdf/2511.18397v1)

[Persona Features Control Emergent Misalignment](https://arxiv.org/html/2506.19823v2)

[Agentic Misalignment: How LLMs could be insider threats](https://www.anthropic.com/research/agentic-misalignment)

[Training large language models on narrow tasks can lead to broad misalignment - Nature](https://www.nature.com/articles/s41586-025-09937-5)

[LLMs behaving badly: mistrained AI models quickly go off the rails](https://www.nature.com/articles/d41586-025-04090-5)

[Emergent Misalignment & Realignment — LessWrong](https://www.lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment)

### Reward Hacking & Goal Misgeneralization

[Goal Misgeneralization in Deep Reinforcement Learning](https://arxiv.org/pdf/2105.14111)

[Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals](https://arxiv.org/abs/2210.01790)

[Goal misgeneralization examples in AI](https://docs.google.com/spreadsheets/d/e/2PACX-1vTo3RkXUAigb25nP7gjpcHriR6XdzA_L5loOcVFj_u7cRAZghWrYKH2L2nU4TA_Vr9KzBX5Bjpz9G_l/pubhtml)

[Specification gaming examples in AI - master list](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)

[Specification gaming: the flip side of AI ingenuity](https://deepmindsafetyresearch.medium.com/specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4)

[Specification gaming examples in AI](https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/)

[How undesired goals can arise with correct rewards](https://deepmind.google/blog/how-undesired-goals-can-arise-with-correct-rewards/)

[Training on Documents about Reward Hacking Induces Reward Hacking](https://alignment.anthropic.com/2025/reward-hacking-ooc/)

[Reinforcement Learning from LLM Feedback to Counteract Goal Misgeneralization](https://arxiv.org/abs/2401.07181)

[Goal Misgeneralization as Implicit Goal Conditioning](https://www.google.com/search?q=Goal+Misgeneralization+as+Implicit+Goal+Conditioning&sourceid=chrome&ie=UTF-8)

[Mitigating Goal Misgeneralization via Minimax Regret](https://arxiv.org/abs/2507.03068)

[2025-Era "Reward Hacking" Does Not Show that Reward Is the Optimization Target — LessWrong](https://www.lesswrong.com/posts/wwRgR3K8FKShjwwL5/2025-era-reward-hacking-does-not-show-that-reward-is-the)

[Reward is not the optimization target — LessWrong](https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target)

[Training a Reward Hacker Despite Perfect Labels — LessWrong](https://www.lesswrong.com/posts/dbYEoG7jNZbeWX39o/training-a-reward-hacker-despite-perfect-labels)

### Alignment Methods

#### RLHF & Preference Optimization

[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)

[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

[RLHF Book](https://rlhfbook.com/book.pdf)

[Reward Generalization in RLHF: A Topological Perspective](https://aclanthology.org/2025.findings-acl.820/)

[PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference](https://arxiv.org/abs/2406.15513)

[BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset](https://arxiv.org/abs/2307.04657)

[Diverse Preference Optimization](https://arxiv.org/pdf/2501.18101v1)

[Language Models Learn to Mislead Humans via RLHF](https://arxiv.org/pdf/2409.12822)

[Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)

[Emergent Alignment via Competition](https://arxiv.org/abs/2509.15090)

[Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)

[Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/pdf/2505.15710)

[Modifying LLM Beliefs with Synthetic Document Finetuning](https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/)

#### Steering Vectors & Activation Engineering

[Model Whisper: Steering Vectors Unlock Large Language Models' Potential in Test-time](https://arxiv.org/abs/2512.04748)

[Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/pdf/2507.21509)

[Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)

[CBMAS: Cognitive Behavioral Modeling via Activation Steering](https://arxiv.org/pdf/2601.06109)

#### Constitutional AI & Value Specification

[Claude's Constitution](https://www.anthropic.com/constitution)

[The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models](https://arxiv.org/pdf/2404.16019)

[Strong and weak alignment of large language models with human values - Scientific Reports](https://www.nature.com/articles/s41598-024-70031-3)

[In situ bidirectional human-robot value alignment](https://www.science.org/stoken/author-tokens/ST-617/full)

[Stress-testing model specs reveals character differences among language models](https://alignment.anthropic.com/2025/stress-testing-model-specs/)

### Safety Evaluation, Benchmarks & Monitoring

[CoT Monitoring.pdf](https://cdn.openai.com/pdf/34f2ada6-870f-4c26-9790-fd8def56387f/CoT_Monitoring.pdf)

[EigenBench: A Comparative Behavioral Measure of Value Alignment](https://arxiv.org/pdf/2509.01938)

[General Scales Unlock AI Evaluation with Explanatory and Predictive Power](https://arxiv.org/pdf/2503.06378)

[SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/pdf/2505.21605)

[Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking](https://arxiv.org/abs/2409.15268)

[Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)

[A benchmark of expert-level academic questions to assess AI capabilities - Nature](https://www.nature.com/articles/s41586-025-09962-4)

[LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output](https://arxiv.org/pdf/2411.06032)

[Introducing EVMbench](https://openai.com/index/introducing-evmbench/)

[Why SWE-bench Verified no longer measures frontier coding capabilities](https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified/)

[Introducing SWE-bench Verified](https://openai.com/index/introducing-swe-bench-verified/)

[HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://www.arxiv.org/pdf/2602.00685)

[On Benchmarking Human-Like Intelligence in Machines](https://arxiv.org/pdf/2502.20502)

[Limitations on Formal Verification for AI Safety — AI Alignment Forum](https://www.alignmentforum.org/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety)

[System Card: Claude Opus 4.6](https://www-cdn.anthropic.com/c788cbc0a3da9135112f97cdf6dcd06f2c16cee2.pdf)

[Auditing AI Bias: The DeepSeek Case](https://dsthoughts.baulab.info/)

### Scalable Oversight & AI Control

[AI Control: Improving Safety Despite Intentional Subversion](https://arxiv.org/abs/2312.06942)

[Scalable Oversight and Weak-to-Strong Generalization: Compatible approaches to the same problem — AI Alignment Forum](https://www.alignmentforum.org/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization)

[Scaling Laws For Scalable Oversight](https://arxiv.org/pdf/2504.18530)

### Conceptual Agendas & Threat Models

[Distributional AGI Safety](https://arxiv.org/abs/2512.16856)

[Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models](https://arxiv.org/pdf/2506.03056)

[Recommendations for Technical AI Safety Research Directions — LessWrong](https://www.lesswrong.com/posts/tG9LGHLzQezH3pvMs/recommendations-for-technical-ai-safety-research-directions)

[Is Power-Seeking AI an Existential Risk?](https://d8737ecf-376e-4788-8d12-a097599c13f6.filesusr.com/ugd/5f37c1_5333aa0b7ff7461abc208b25bfc7df87.pdf)

[Optimal Policies Tend to Seek Power](https://arxiv.org/pdf/1912.01683)

[Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development](https://arxiv.org/pdf/2501.16946)

[Gradual Disempowerment](https://gradual-disempowerment.ai/)

[Orienting to 3 year AGI timelines — LessWrong](https://www.lesswrong.com/posts/jb4bBdeEEeypNkqzj/orienting-to-3-year-agi-timelines)

[Schelling fences on slippery slopes — LessWrong](https://www.lesswrong.com/posts/Kbm6QnJv9dgWsPHQP/schelling-fences-on-slippery-slopes)

[Gandhi, murder pills, and mental illness — LessWrong](https://www.lesswrong.com/posts/SdkAesHBt4tsivEKe/gandhi-murder-pills-and-mental-illness)

[The Shard Theory of Human Values](https://turntrout.com/shard-theory)

[Shard Theory in Nine Theses: a Distillation and Critical Appraisal — LessWrong](https://www.lesswrong.com/posts/8ccTZ9ZxpJrvnxt4F/shard-theory-in-nine-theses-a-distillation-and-critical)

[The Shard Theory of Human Values – Center for Human-Compatible Artificial Intelligence](https://humancompatible.ai/news/2022/10/14/the-shard-theory-of-human-values/)

[Fitness-Seekers: Generalizing the Reward-Seeking Threat Model — LessWrong](https://www.lesswrong.com/posts/bhtYqD4FdK6AqhFDF/fitness-seekers-generalizing-the-reward-seeking-threat-model)

[The behavioral selection model for predicting AI motivations — LessWrong](https://www.lesswrong.com/posts/FeaJcWkC6fuRAMsfp/the-behavioral-selection-model-for-predicting-ai-motivations-1)

[Simulators — LessWrong](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators)

[Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover — LessWrong](https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to)

[Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844)

[Technical Report: Evaluating Goal Drift in Language Model Agents](https://arxiv.org/abs/2505.02709)

[Part 1: 4. Where can misaligned goals come from?](https://www.youtube.com/watch?v=KKMETIVEzXA&list=PLw9kjlF6lD5UqaZvMTbhJB8sV-yuXu5eW&index=8)

## AI Ethics, Governance & Policy

### Value Alignment & Social Choice

[Values in the wild: Discovering and analyzing values in real-world language model interactions](https://www.anthropic.com/research/values-wild)

[Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback](https://arxiv.org/pdf/2404.10271)

[OpenAI x DFT: The First Moral Graph](https://meaningalignment.substack.com/p/the-first-moral-graph)

[Eliciting Human Preferences with Language Models](https://arxiv.org/pdf/2310.11589)

[Democratic AI is Possible. The Democracy Levels Framework Shows How It Might Work](https://arxiv.org/pdf/2411.09222)

[AI can help humans find common ground in democratic deliberation](https://www.science.org/doi/10.1126/science.adq2852)

[Generative Social Choice](https://arxiv.org/pdf/2309.01291)

[Ethical Issues In Advanced Artificial Intelligence](https://nickbostrom.com/ethics/ai)

[Taking AI Welfare Seriously](https://arxiv.org/pdf/2411.00986)

[A roadmap for evaluating moral competence in large language models - Nature](https://www.nature.com/articles/s41586-025-10021-1#:~:text=The%20question%20of%20whether%20large,actions%20on%20behalf%20of%20humans.)

[Moral Judgments of Human vs. AI Agents in Moral Dilemmas](https://pmc.ncbi.nlm.nih.gov/articles/PMC9951994/)

[Fair algorithms for selecting citizens' assemblies - Nature](https://www.nature.com/articles/s41586-021-03788-6)

### Governance & Regulation

[NY State RAISE Act](https://www.nysenate.gov/legislation/bills/2025/A6453)

[Regulatory Markets: The Future of AI Governance](https://arxiv.org/pdf/2304.04914)

[Democratic inputs to AI grant program: lessons learned and implementation plans](https://openai.com/index/democratic-inputs-to-ai-grant-program-update/)

[Artificial Intelligence and Democracy: European Parliamentary Technology Assessment Network Report 2024](https://www.eptanetwork.org/images/documents/EPTA_Report_on_AI_and_Democracy_FINAL.pdf)

### AI Misuse, Security & Adversarial Attacks

[Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/pdf/2510.07192)

[A small number of samples can poison LLMs of any size](https://www.anthropic.com/research/small-samples-poison)

[Poisoning Web-Scale Training Datasets is Practical](https://arxiv.org/abs/2302.10149)

[Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/pdf/2512.09742)

[Discovering Forbidden Topics in Language Models](https://arxiv.org/pdf/2505.17441)

[Disrupting malicious uses of AI by state-affiliated threat actors](https://openai.com/index/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors/)

[Detecting and countering misuse of AI: August 2025](https://www.anthropic.com/news/detecting-countering-misuse-aug-2025)

[LLMs unlock new paths to monetizing exploits](https://arxiv.org/pdf/2505.11449)

[Scalable watermarking for identifying large language model outputs - Nature](https://www.nature.com/articles/s41586-024-08025-4)

### AI & Society

[The TESCREAL Bundle](https://www.dair-institute.org/projects/tescreal/)

[The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/html/2506.06166)

[The ghost of behaviorism: critical reflections on methodological limitations in the research of large language models psychology - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S1389041726000124)

[ELIZA effect - Wikipedia](https://en.wikipedia.org/wiki/ELIZA_effect#cite_note-Billings1-6)

## Human-AI Interaction & Comparison

### Cognitive Comparisons

[Shared sensitivity to data distribution during learning in humans and transformer networks - Nature Human Behaviour](https://www.nature.com/articles/s41562-025-02359-3)

[Human-like object concept representations emerge naturally in multimodal large language models - Nature Machine Intelligence](https://www.nature.com/articles/s42256-025-01049-z)

[Parallels between human and artificial minds when new learning erases old knowledge - Nature Human Behaviour](https://www.nature.com/articles/s41562-025-02326-y)

[A large-scale comparison of divergent creativity in humans and large language models - Nature Human Behaviour](https://www.nature.com/articles/s41562-025-02331-1)

[Aligning Machine and Human Visual Representations across Abstraction Levels](https://arxiv.org/abs/2409.06509)

[LLMs differ from human cognition because they are not embodied - Nature Human Behaviour](https://www.nature.com/articles/s41562-023-01723-5)

[Inter-brain neural dynamics in biological and artificial intelligence systems - Nature](https://www.nature.com/articles/s41586-025-09196-4)

[Temporal structure of natural language processing in the human brain corresponds to layered hierarchy of large language models - Nature Communications](https://www.nature.com/articles/s41467-025-65518-0)

### Impact on Skills, Behavior & Cognition

[How AI assistance impacts the formation of coding skills](https://www.anthropic.com/research/AI-assistance-coding-skills)

[How AI Impacts Skill Formation](https://arxiv.org/pdf/2601.20245)

[How human–AI feedback loops alter human perceptual, emotional and social judgements - Nature Human Behaviour](https://www.nature.com/articles/s41562-024-02077-2)

[Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/pdf/2506.08872v1)

[Human–AI collaboration enables more empathic conversations in text-based peer-to-peer mental health support - Nature Machine Intelligence](https://www.nature.com/articles/s42256-022-00593-2)

[Deliberate Lab: A Platform for Real-Time Human-AI Social Experiments](https://arxiv.org/pdf/2510.13011)

### Persuasion & Communication

[LLM-generated messages can persuade humans on policy issues - Nature Communications](https://www.nature.com/articles/s41467-025-61345-5)

[Can AI Change Your View? Evidence from a Large-Scale Online Field Experiment](https://regmedia.co.uk/2025/04/29/supplied_can_ai_change_your_view.pdf)

[Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/html/2510.03215v1)

## Deep Learning

### Architectures

[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

[Attention Is Not What You Need](https://arxiv.org/abs/2512.19428)

[Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)

[Nested Learning: The Illusion of Deep Learning Architectures](https://arxiv.org/abs/2512.24695)

[An Analogy for Understanding Transformers — LessWrong](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)

[Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605)

### Training Methods

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

[A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)

[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)

[Generative Modeling via Drifting](https://arxiv.org/pdf/2602.04770)

[On-Policy Distillation](https://thinkingmachines.ai/blog/on-policy-distillation/)

### Learning Theory & Generalization

[Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)

[Deep Networks Always Grok and Here is Why](https://arxiv.org/pdf/2402.15555)

[AI models collapse when trained on recursively generated data - Nature](https://www.nature.com/articles/s41586-024-07566-y)

[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)

[Shortcut learning in deep neural networks - Nature Machine Intelligence](https://www.nature.com/articles/s42256-020-00257-z)

[The Pitfalls of Simplicity Bias in Neural Networks](https://arxiv.org/abs/2006.07710)

[Notes on "An Observation on Generalization" | Sumanth](https://sumanthrh.com/post/notes-on-generalization/)

[Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning](https://arxiv.org/abs/2507.16795)

[Learning to Reason in 13 Parameters](https://arxiv.org/pdf/2602.04118)

[The Neural Net Tank Urban Legend](https://gwern.net/tank#alternative-examples)

### Optimization

[Preconditioned inexact stochastic ADMM for deep models - Nature Machine Intelligence](https://www.nature.com/articles/s42256-026-01182-3)

[Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models](https://arxiv.org/abs/2505.18230)

### Applications & Systems

[Accurate predictions on small data with a tabular foundation model - Nature](https://www.nature.com/articles/s41586-024-08328-6)

[Generating conjectures on fundamental constants with the Ramanujan Machine - Nature](https://www.nature.com/articles/s41586-021-03229-4)

[KaLM-Embedding - Multilingual Text Embedding Models](https://kalm-embedding.github.io/)

[KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/pdf/2506.20923)

[GitHub - google-research/timesfm: TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.](https://github.com/google-research/timesfm/)

[TOPS-speed complex-valued convolutional accelerator for feature extraction and inference - Nature Communications](https://www.nature.com/articles/s41467-024-55321-8)

[Optimal solid state neurons - Nature Communications](https://www.nature.com/articles/s41467-019-13177-3)

[Latent Collaboration in Multi-Agent Systems](https://arxiv.org/pdf/2511.20639)

[Human-level play in the game of Diplomacy by combining language models with strategic reasoning](https://www.science.org/doi/10.1126/science.ade9097)

[Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/pdf/2505.22954)

[Large language model powered knowledge graph construction for mental health exploration - Nature Communications](https://www.nature.com/articles/s41467-025-62781-z)

## Reinforcement Learning

[The 37 Implementation Details of Proximal Policy Optimization · The ICLR Blog Track](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

[1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities](https://arxiv.org/pdf/2503.14858)

[The History and Risks of Reinforcement Learning and Human Feedback](https://arxiv.org/pdf/2310.13595)

[Lifelong Reinforcement Learning via Neuromodulation](https://arxiv.org/pdf/2408.08446?)

[Champion-level drone racing using deep reinforcement learning - Nature](https://www.nature.com/articles/s41586-023-06419-4)

[Discovery of the reward function for embodied reinforcement learning agents - Nature Communications](https://www.nature.com/articles/s41467-025-66009-y)

[Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning](https://arxiv.org/abs/2408.10075)

[Bridging the human–AI knowledge gap through concept discovery and transfer in AlphaZero](https://www.pnas.org/doi/10.1073/pnas.2406675122)

[TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)

[Distributional Reinforcement Learning](https://www.distributional-rl.org/)

[PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)

[In-Context Reinforcement Learning](https://www.google.com/search?q=in%20context%20reinforcement%20learning&sourceid=chrome&ie=UTF-8)

## Neuroscience

### Computational Neuroscience & Neural Manifolds

[Infinite hidden Markov models can dissect the complexities of learning - Nature Neuroscience](https://www.nature.com/articles/s41593-025-02130-x)

[A neural manifold view of the brain - Nature Neuroscience](https://www.nature.com/articles/s41593-025-02031-z)

[How distributed is the brain-wide network that is recruited for cognition? - Nature Reviews Neuroscience](https://www.nature.com/articles/s41583-025-00992-5)

[TransBrain: a computational framework for translating brain-wide phenotypes between humans and mice - Nature Methods](https://www.nature.com/articles/s41592-025-02961-3)

### Reward, Learning & Decision-Making

[Dopamine neurons report an error in the temporal prediction of reward during learning - Nature Neuroscience](https://www.nature.com/articles/nn0898_304)

[Predictive coding of reward in the hippocampus - Nature](https://www.nature.com/articles/s41586-025-09958-0)

[Hybrid neural–cognitive models reveal how memory shapes human reward learning - Nature Human Behaviour](https://www.nature.com/articles/s41562-025-02324-0)

[Evidence accumulation from experience and observation in the cingulate cortex - Nature](https://www.nature.com/articles/s41586-025-09885-0)

### Memory & Sleep

[A neural state space for episodic memories](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(25)00284-0)

[A hippocampal 'sharp-wave sleep' state that is dissociable from cortical sleep - Nature Neuroscience](https://www.nature.com/articles/s41593-025-02141-8)

### Emotion, Stress & Pharmacology

[Understanding the neural code of stress to control anhedonia - Nature](https://www.nature.com/articles/s41586-024-08241-y)

[Dopamine pathways mediating affective state transitions after sleep loss - PubMed](https://pubmed.ncbi.nlm.nih.gov/37922904/)

[Psilocybin triggers an activity-dependent rewiring of large-scale cortical networks](https://www.cell.com/cell/fulltext/S0092-8674(25)01305-4)

[The nature of feelings: evolutionary and neurobiological origins - Nature Reviews Neuroscience](https://www.nature.com/articles/nrn3403)

[Adenosine signalling drives antidepressant actions of ketamine and ECT - Nature](https://www.nature.com/articles/s41586-025-09755-9)

### Brain Structure, Development & Connectivity

[The Information Theory of Aging - Nature Aging](https://www.nature.com/articles/s43587-023-00527-6)

[Topological turning points across the human lifespan - Nature Communications](https://www.nature.com/articles/s41467-025-65974-8)

### Language & Perception

[Combined evidence from artificial neural networks and human brain-lesion models reveals that language modulates vision in human perception - Nature Human Behaviour](https://www.nature.com/articles/s41562-025-02357-5)

## Psychology

### Social & Cultural

[Group membership biases children's evaluation of evidence - Nature Communications](https://www.nature.com/articles/s41467-025-66085-0)

[How people think about being alone shapes their experience of loneliness - Nature Communications](https://www.nature.com/articles/s41467-025-56764-3)

[People quasi-randomly assigned to farm rice are more collectivistic than people assigned to farm wheat - Nature Communications](https://www.nature.com/articles/s41467-024-44770-w)

[The Church, intensive kinship, and global psychological variation](https://www.science.org/doi/10.1126/science.aau5141)

[Bridging the Gap: Enhancing Empathy Perceptions Fosters Social Connection](https://osf.io/u584x/files/zcmpn)

[Bridging the empathy perception gap fosters social connection - Nature Human Behaviour](https://www.nature.com/articles/s41562-025-02307-1.pdf)

[Need for cognitive closure predicts preference for similar others and reduced diversity in social networks - Scientific Reports](https://www.nature.com/articles/s41598-026-36288-6)

[Recent discoveries on the acquisition of the highest levels of human performance](https://www.science.org/doi/10.1126/science.adt7790)

### Moral Psychology

[Does Morality Do Us Any Good?](https://www.newyorker.com/magazine/2024/12/30/the-invention-of-good-and-evil-hanno-sauer-book-review)

[The emotional dog and its rational tail: a social intuitionist approach to moral judgment - PubMed](https://pubmed.ncbi.nlm.nih.gov/11699120/)

### Political Psychology

[Clarifying the Structure and Nature of Left-Wing Authoritarianism](https://osf.io/preprints/psyarxiv/3nprq_v1)

[Liberals Think More Analytically (More "WEIRD") Than Conservatives](https://journals.sagepub.com/doi/abs/10.1177/0146167214563672?__cf_chl_tk=RD6d9Z4jCwvcJAo1CJ94vaZtgwa_nzAIRVRtNY3w.0A-1768663477-1.0.1.1-d8thO2YsckmWlNEXRsWlbQdhEoq.BDO0UTnblHhwnp8)

[Liberals Read, Conservatives Watch TV](https://www.richardhanania.com/p/liberals-read-conservatives-watch)

### Clinical & Individual Differences

[Mysterious illnesses have supernatural and ritualistic cures: Evidence from 3,655 century-old Irish folk cures - PubMed](https://pubmed.ncbi.nlm.nih.gov/41325525/)

[No meta-analytical effect of economic inequality on well-being or mental health - Nature](https://www.nature.com/articles/s41586-025-09797-z)

[Individual differences in need for cognitive closure.](https://psycnet.apa.org/doiLanding?doi=10.1037/0022-3514.67.6.1049)

[Motivated closing of the mind: "seizing" and "freezing" - PubMed](https://pubmed.ncbi.nlm.nih.gov/8637961/)

### Judgment & Decision-Making

[Conservatism in a simple probability inference task.](https://psycnet.apa.org/record/1966-11887-001)

[Using large-scale experiments and machine learning to discover theories of human decision-making](https://www.science.org/doi/10.1126/science.abe2629)

## Cognitive Science

### Bayesian Cognition

[Bayesian Models of Cognition](https://oecs.mit.edu/pub/lwxmte1p/release/2)

[Word learning as Bayesian inference](https://escholarship.org/content/qt8b67v4q2/qt8b67v4q2_noSplash_c575822e1e460119556d379406f0f24e.pdf)

[Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)

[Bayes and blickets: Effects of knowledge on causal induction in children and adults](https://pmc.ncbi.nlm.nih.gov/articles/PMC3208735/)

[Bridging Levels of Analysis for Probabilistic Models of Cognition](https://cocosci.princeton.edu/tom/papers/LabPublications/BridgingLevelsAnalysis.pdf)

[How to Grow a Mind: Statistics, Structure, and Abstraction](https://www.science.org/doi/10.1126/science.1192788)

[Grounding Language about Belief in a Bayesian Theory-of-Mind](https://arxiv.org/pdf/2402.10416)

[Principle of maximum entropy - Wikipedia](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy)

### Causal Reasoning & Rational Models

[Children's causal inferences from indirect evidence: Backwards blocking and Bayesian reasoning in preschoolers - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0364021303001149)

[Rational Approximations to Rational Models: Alternative Algorithms for Category Learning](https://cocosci.princeton.edu/tom/papers/rationalapproximations.pdf)

[Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality](https://arxiv.org/pdf/2505.19376)

### Learning, Concepts & Language

[Building Machines That Learn and Think Like People](https://arxiv.org/abs/1604.00289)

[What does it mean to understand language?](https://arxiv.org/abs/2511.19757)

[Fuzzy-Trace Theory - an overview | ScienceDirect Topics](https://www.sciencedirect.com/topics/neuroscience/fuzzy-trace-theory)

[Fuzzy Trace Theory](https://www.google.com/search?q=fuzzy+trace+theory&ie=UTF-8&oe=UTF-8&hl=zh-hans-us&client=safari&authuser=1)

[Humans Select Subgoals That Balance Immediate and Future Cognitive Costs During Physical Assembly](https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.70135)

[When is better to think without words?](https://www.henrikkarlsson.xyz/p/wordless-thought?r=bh6vo&triedRedirect=true)

## Philosophy of Mind

### Consciousness & Subjective Experience

[Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)

[Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/pdf/2308.08708)

[Out of Our Heads: Why You Are Not Your Brain, and Other Lessons from the Biology of Consciousness.](https://www.google.com/search?q=Out+of+Our+Heads%3A+Why+You+Are+Not+Your+Brain%2C+and+Other+Lessons+from+the+Biology+of+Consciousness.&ie=UTF-8&oe=UTF-8&hl=zh-hans-us&client=safari)

[This new tool could tell us how consciousness works](https://news.mit.edu/2026/new-tool-could-tell-us-how-consciousness-works-0112)

[A quantum experiment suggests there's no such thing as objective reality – MIT Technology Review](https://www.technologyreview.com/2019/03/12/136684/a-quantum-experiment-suggests-theres-no-such-thing-as-objective-reality/amp/)

[Conscious artificial intelligence and biological naturalism](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/C9912A5BE9D806012E3C8B3AF612E39A/S0140525X25000032a.pdf/conscious-artificial-intelligence-and-biological-naturalism.pdf)

### Mind, Representation & Computation

[Brains in a Vat (Stanford Encyclopedia of Philosophy/Spring 2010 Edition)](https://plato.stanford.edu/archIves/spr2010/entries/brain-vat/)

[Gödel, Escher, Bach - Wikipedia](https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach)

[The free-energy principle: a unified brain theory? - Nature Reviews Neuroscience](https://www.nature.com/articles/nrn2787)

### Epistemology of AI Mind Claims

[Agnosticism About Artificial Consciousness](https://arxiv.org/abs/2412.13145)

[Illusions of AI consciousness](https://www.science.org/doi/10.1126/science.adn4935)

## Politics & Current Events

### China

[Why China Is Suddenly Obsessed With American Poverty](https://www.nytimes.com/2026/01/13/business/china-american-poverty.html)

[China obsesses over America's "kill line"](https://www.economist.com/china/2026/01/12/china-obsesses-over-americas-kill-line)

[Large-Scale Psychological Differences Within China Explained by Rice Versus Wheat Agriculture](https://www.science.org/doi/10.1126/science.1246850)

[Moving chairs in Starbucks: Observational studies find rice-wheat cultural differences in daily life in China](https://www.science.org/doi/10.1126/sciadv.aap8469)

[Earthbound China: A Study of Rural Economy in Yunnan](https://www.google.com/search?q=Earthbound+China:+A+Study+of+Rural+Economy+in+Yunnan&ie=UTF-8&oe=UTF-8&hl=zh-hans-us&client=safari&authuser=1)

[Taiwan: Asia's Orphan?](https://www.nbr.org/wp-content/uploads/pdfs/publications/special_report_62_taiwan_asiasorphan_december2016.pdf)

[亚细亚的孤儿](https://www.google.com/search?q=%E4%BA%9A%E7%BB%86%E4%BA%9A%E7%9A%84%E5%AD%A4%E5%84%BF&ie=UTF-8&oe=UTF-8&hl=zh-hans-us&client=safari)

[How Censorship in China Allows Government Criticism but Silences Collective Expression | American Political Science Review | Cambridge Core](https://www.cambridge.org/core/journals/american-political-science-review/article/abs/how-censorship-in-china-allows-government-criticism-but-silences-collective-expression/C7EF4A9C9D59425C2D09D83742C1FE00)

[Reverse-engineering censorship in China: Randomized experimentation and participant observation](https://www.science.org/doi/epdf/10.1126/science.1251722)

[How Censorship in China Allows Government Criticism but Silences Collective Expression](https://gking.harvard.edu/sites/g/files/omnuum7116/files/gking/files/censored.pdf)

### Japan 2026 Election

[Japan's thunderbolt election: Takaichi resets politics, economics, and diplomacy | Brookings](https://www.brookings.edu/articles/japans-thunderbolt-election-takaichi-resets-politics-economics-and-diplomacy/)

[Takaichi Dominates Japan's Lower House Election](https://www.csis.org/analysis/takaichi-dominates-japans-lower-house-election)

[Japan's Historic Election: Insights from Hudson Experts](https://www.hudson.org/global-economy/japans-historic-election-insights-hudson-experts-weistein-chou-murano-pryztup-sracic)

[Japan's Takaichi to pursue conservative agenda after election landslide](https://www.npr.org/2026/02/09/nx-s1-5706602/japan-takaichi-conservative-agenda-election-landslide)

[Japanese Prime Minister Sanae Takaichi hopes for big win as polls open in national elections](https://www.cnbc.com/2026/02/07/sanae-takaichi-japan-snap-election.html)

[Why Japan's New PM Is Calling a Snap Election](https://time.com/7346495/japan-sanae-takaichi-snap-election-ldp/)

[Japan's Takaichi Wins Big in Snap Election: What to Know](https://time.com/7372956/japan-takaichi-election-win/)

[She's one of the world's most powerful conservative leaders – and she just won again | CNN](https://edition.cnn.com/2026/02/09/asia/sanae-takaichi-japan-election-win-intl-hnk)

[Japan's Sanae Takaichi wins a supermajority after gambling on a snap election](https://www.nbcnews.com/world/japan/japans-sanae-takaichi-wins-landslide-snap-election-exit-polls-project-rcna257887)

### Democracy, Media & Public Opinion

[Opinion | OpenAI Is Making the Mistakes Facebook Made. I Quit.](https://www.nytimes.com/2026/02/11/opinion/openai-ads-chatgpt.html)

[Like-minded sources on Facebook are prevalent but not polarizing - Nature](https://www.nature.com/articles/s41586-023-06297-w)

[How do social media feed algorithms affect attitudes and behavior in an election campaign?](https://www.science.org/doi/10.1126/science.abp9364)

### US Politics

[Manhattan Assemblyman Joins Primary to Succeed Nadler in Congress](https://www.nytimes.com/2025/10/20/nyregion/alex-bores-ny-congress-primary.html)

## Historical & Classic Texts

[The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

[Singularity – Eliezer S. Yudkowsky](https://www.yudkowsky.net/singularity)

[HOW LONG BEFORE SUPERINTELLIGENCE?](https://www.eecs.ucf.edu/~lboloni/Teaching/CAP5636_Fall2023/homeworks/Reading%202%20-%20Nick%20Bostrom-How%20long%20before%20superintelligence.pdf)

[A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)

[The Dragons of Eden](https://www.google.com/search?q=carl+sagan+dragons+of+eden&ie=UTF-8&oe=UTF-8&hl=zh-hans-us&client=safari&authuser=1)

[Black Skin White Masks](https://www.google.com/search?q=black+skin+white+masks&ie=UTF-8&oe=UTF-8&hl=zh-hans-us&client=safari)

[RELIGION AND SCIENCE (Published 1930)](https://www.nytimes.com/1930/11/09/archives/religion-and-science.html)

[How a Huguenot philosopher realised that atheists could be virtuous | Aeon Ideas](https://aeon.co/ideas/how-a-huguenot-philosopher-realised-that-atheists-could-be-virtuous)

[Eichmann in Jerusalem: A Report on the Banality of Evil](https://www.google.com/search?q=Eichmann+in+Jerusalem:+A+Report+on+the+Banality+of+Evil&sourceid=chrome&ie=UTF-8)

[Stanford prison experiment - Wikipedia](https://en.wikipedia.org/wiki/Stanford_prison_experiment)

[Interpersonal Dynamics in a Simulated Prison](http://pdf.prisonexp.org/ijcp1973.pdf)

## Cross-Disciplinary Essays

[Could a Neuroscientist Understand a Microprocessor?](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268)

[Can a biologist fix a radio?—Or, what I learned while studying apoptosis](https://www.cell.com/cancer-cell/fulltext/S1535-6108(02)00133-2?_returnURL=https://linkinghub.elsevier.com/retrieve/pii/S1535610802001332?showall%3Dtrue)

[Machine behaviour - Nature](https://www.nature.com/articles/s41586-019-1138-y)

[Why I'm not a Bayesian — LessWrong](https://www.lesswrong.com/posts/TyusAoBMjYzGN3eZS/why-i-m-not-a-bayesian)

[AI & Externalizing Knowledge](https://atharvanihalani.substack.com/p/ai-and-externalizing-knowledge)

[False Faces – Sinceriously](https://sinceriously.blog-mirror.com/false-faces/)

## Resources & Meta

### Courses, Tutorials & Lecture Notes

[NeurIPS 2025: The Science of Benchmarking Tutorial](https://benchmarking.science/)

[Welcome to Spinning Up in Deep RL! — Spinning Up documentation](https://spinningup.openai.com/en/latest/index.html)

[Tutorial 2: Introduction to PyTorch — UvA DL Notebooks v1.2 documentation](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html)

[Deep Learning Course - Neuromatch](https://neuromatch.io/deep-learning-course/)

[Introduction — Neuromatch Academy: Deep Learning](https://deeplearning.neuromatch.io/tutorials/intro.html)

[Mind: Introduction to Cognitive Science](https://bayanebartar.org/file-dl/library/Linguistic/Mind-Introduction-to-Cognitive-Science%20.pdf)

[Lecture 9: Hacker's Guide to Deep Learning | Deep Learning | Electrical Engineering and Computer Science | MIT OpenCourseWare](https://ocw.mit.edu/courses/6-7960-deep-learning-fall-2024/resources/mit6_7960_f24_lec9_pdf/)

[Harvard MLD 360: Being Human](https://marshallganz.scholars.harvard.edu/sites/g/files/omnuum6421/files/2025-03/Being%20Human%202025%20Syllabus%20%28Final%29_0.pdf)

### Reading Lists, Glossaries & Open Problems

[An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers v2 — AI Alignment Forum](https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite-1)

[200 Concrete Open Problems in Mechanistic Interpretability](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit?tab=t.0#heading=h.n514s7caro7u)

[A Comprehensive Mechanistic Interpretability Explainer & Glossary - Dynalist](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_Jzi6YHRHKP1JziwdE02qdYZ)

[AI alignment resources](https://vkrakovna.wordpress.com/ai-safety-resources/)

[AI Control Reading Group (May '25)](https://docs.google.com/document/d/1dBlzu0xB-9KJumixzi-9AIZATyKq-U66HylioqXy4JI/preview?tab=t.0#heading=h.lgeivf4krkvn)

### Labs, Organizations & Projects

[Victoria Krakovna](https://vkrakovna.wordpress.com/blog/page/2/)

[Newspeak House](https://newspeak.house/#events)

[Robyn](https://meetrobyn.com/)

[DAIR Institute](https://www.dair-institute.org/)

[HumanLM](https://humanlm.stanford.edu/)

[Elicit: AI for scientific research](https://elicit.com/)

[Meaning Alignment Institute](https://www.meaningalignment.org/)

[cloud.cb](https://cloudcb.me/)

[Detail · Where craft lives](https://detail.design/)

## News & Commentary

[Why AI chatbots lie to us](https://www.science.org/doi/10.1126/science.aea3922)

[Does AI already have human-level intelligence? The evidence is clear](https://www.nature.com/articles/d41586-026-00285-6)

[Claude Opus 4.6](https://www.anthropic.com/news/claude-opus-4-6)

[In the age of AI, we need a human-centered society more than ever](https://mres.medium.com/in-the-age-of-ai-we-need-a-human-centered-society-more-than-ever-0be3449ad40e)

[The AI's Existential Crisis: An Unexpected Journey with Cursor and Gemini 2.5 Pro](https://medium.com/@sobyx/the-ais-existential-crisis-an-unexpected-journey-with-cursor-and-gemini-2-5-pro-7dd811ba7e5e)

[A mask off moment for Anthropic and Dario Amodei](https://www.reddit.com/r/ArtificialInteligence/comments/1oxr824/a_mask_off_moment_for_anthropic_and_dario_amodei/)

[welcome to summitbridge, an extremely normal company](https://nostalgebraist.tumblr.com/post/787119374288011264/welcome-to-summitbridge)

[the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)

[OpenAI makes OpenClaw inexpensive](https://x.com/andrewwarner/status/2024518232271417434?s=46)

[OpenClaw might just be the beginning of Web 3.0?  (no, not the crypto web3)](https://x.com/championswimmer/status/2024289849881330040?s=46)

[Deloitte Investigation | SomaliScan](https://www.somaliscan.com/investigations/deloitte)

[About | SomaliScan](https://www.somaliscan.com/about)

## Miscellaneous

[Principal Component Analyses (PCA)-based findings in population genetic studies are highly biased and must be reevaluated - Scientific Reports](https://www.nature.com/articles/s41598-022-14395-4)

[Used Citroen 2 CV Cars For Sale | Autotrader UK](https://www.autotrader.co.uk/cars/used/citroen/2-cv?refresh=true)
